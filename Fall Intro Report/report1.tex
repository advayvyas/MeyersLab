\documentclass[12pt]{article}
\usepackage{../advay} 
\usepackage{xcolor} 
\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13} 
\definecolor{RubineRed}{rgb}{0.8, 0.0, 0.3} 
\definecolor{RoyalBlue}{rgb}{0.25, 0.41, 0.88}
\usepackage{graphicx}
\usepackage{biblatex}
\addbibresource{refs.bib}

\title{Meyers Lab Report \#1}
\author{Advay Vyas}
\date{\today}

\begin{document}
\maketitle

\tableofcontents
\section{Forecast evaluation background}
I plan to read~\cite{hewamalage23} in its entirety and take notes to learn how forecast evaluation works.
\subsection{Introduction}
ML metrics are very different than forecasting metrics because time series data is much messier and the regular ways of determining model success fail to measure accurately. Forecast origin is self-explanatory and forecast horizon is the section of time that we are predicting upon. Fixed origin evaluation uses the same training data each iteration and the forecasts are computed ``one-step ahead''. On the other hand, rolling origin evaluation incorporates the new data into the testing set first, and then into the training set on the next iteration. In my opinion, rolling origin seems a lot better and I think that is what we use -- not sure yet though. 

Time series data also often has a series of issues that make predicting and measuring predictions much harder. Time series can have non-stationarities like seasonality, trends, or breaks; non-normality like fat tails and outliers; and series with a very short history which are inherently hard to use as training data. 

\subsection{Benchmarks and exaggerating success}
Next, the paper moves on to the topic of benchmarks. The naive forecast (a.k.a no-change model) uses the last known observation as the forecast and actually has good performance in some scenarios (likely those with little to no change period to period, not for us with the prevalance of vaccine skepticism). While not a viable model for real prediction, it should definitely be used a strong benchmark to check the performance of other models. Rather than an abstract metric, performing better than the most simple prediction should be the bare minimum for an accurate model. For example, finance models sometimes involve heavy computations like neural networks only to fail against the naive forecast (random walk w/out drift). Essentially, ML research fails to account to be better than simple random modeling -- benchmarks are useful!

In more complex scenarios like ``clear seasonal patterns'', a seasonal naive model should be used - makes sense. Even though that seems obvious, researchers often compare to non-seasonal benchmarks and can show great results. To add more to this discussion, papers often use ``overkill'' ways of forecasting for no reason - the paper here mentions a modeling a 2-dimensional linear relationship with a neural network (page 801). 

\subsection{Evaluation metrics, plots, and data leakage}
Metrics like MSE, RMSE, MAE are often used with many different time series yet it is very important to watch out for each time series having a different scale. Some (like myself last semester) have used the $R^2$ value to determine how well a model predicts (especially in random walks, while mine was definitely not random). MAPE is also another metric - luckily for us, we are going to be using a weighted interval score (next section!). 

Forecasting plots are also another area to watch out for because they can be quite confusing when comparing different models. For example, watching fit by looking at the ``horizontal'' shift and finding it to be small instead of the ``vertical'' shift can give the model the illusion of fitting well. This paper makes the suggestion to only use plots of the forecasts for sanity checks and not for real use. I disagree and I think that those horizontal shifts imply that the model can predict spikes and dips after a delay (as long as it's about always the same delay) and that plots serve a very useful purpose of pinpointing far-off predictions without requiring brute force checking.

Lastly, we tackle the topic of data leakage (usage of the test/unseen data during the training process). In forecasting, since it involves time, it is often to hard to keep track of data seperation during rolling origin prediction. While that is unavoidable, indirect forms of data leakage are much more common. For example, in forecasting, smoothing, decomposition, or normalization over the series before prediction can indirectly help the model predict where the missing data will fit into the distribution. With data leakage, models can often easily outperform any existing or new frameworks, causing issues in result accuracy. This problem also arises when using multiple time series at once, where one series could help the model predict the entire outcome. In conclusion, the most important way to avoid data leakage are during preprocessing, feature extraction, and making sure that one series doesn't reveal the future of another series.

\subsection{Guidelines and best practices}
The paper states that the forecast model construction plus evaluation usually contains these steps:
\begin{itemize}
    \item Data forecasting
    \item Forecasting
    \item Error calculation
    \item Error measure calculation
    \item Statistical tests for significance
\end{itemize}

\subsubsection{Data partitioning}
The paper now examines guidelines of data partitioning. Fixed origin setup is one of the fastest evaluation setups since no new training data is added. However, with single series forecasting, the setup only provides one forecast. It is recommended to have multiple forecasts instead to see the forecast distribution. A drawback of fixed origin setup is that errors might arise from the patterns in that particular region. 

On the other hand, rolling origin (a.k.a. time series cross-validation) evaluation setups update the forecast origin every step and the model encounters new actual data. The two options available now are to recalibrate with new data completely or update the model as input. Traditional models normally recalibrate while ML models just accept new data and periodically retrain the model due to it not being an option. Rolling origin can be conducted through either expanding the window or rolling the window setup. Expanding the window retains all the data from the very beginning, effectively just slowly growing the total training data. Alternatively, a rolling window setup makes sure that the data ``length'' stays the same - the time period length as the old data is deleted to make room for the new data. It is also possible to start with an expanding window and then transition to a rolling window as needed.

A common misconception is that temporal order is important for the time series during training. A form of data partitioning that utilizes this fact is k-fold cross-validation, which splits up the data into different ``folds'' randomly and repartitions training and test data. However, it does have a couple drawbacks like causing problems with non-stationarities, difficulty in capturing serial correlation, and training data containing future data while test data contains old data. Even with these problems, pure AR models can benefit from this technique. Additionally, data-partitioning for non-stationary data is very hard (the paper is quite vague on this, around page 811). 

In conclusion, data partitioning boils down to the length of the series (kind of), where short series choose k-fold CV while the more common longer series use tsCV (time-series cross-validation).

\subsubsection{Error measures}
Error measures are a key focus of this week's work so this section should be particularly important. Scale-dependent forecast bias can be assessed with Mean Error, while two other scale-dependent measures usually used in regression are MSE and MAE. However, we should use scale-independent (next to impossible) but we will examine a wide variety of measures. A couple examples are MSE and RMSE that optimize for the mean, MAE and MASE that optimize for the median, and some researchers apply a mix of these error models. Actually, the mix/blend of these models is the most optimal method as of now and is highly recommended. I think we will be doing weighted interval score but I'm curious to see how it is similar/different to mixed error methods in time-series data. On pages 814--815, various common error measures are listed. On pages 816--820, a comphrensive list of error measures is listed by category for reference. 

\subsubsection{Statistical significance tests}
The Diebold-Mariano test and Wilcoxon rank-sum test are two major tets for comparing between two competing forecasts. Another two-forecast test is the Giacomini-White test that can also assess the conditional predictive ability. A table of statistical signifiance tests and associated information follows from pages 822--825. Moreover, the Friedman test looks at detecting signifiance between multiple comepting methods, looking at the ranks of the methods sorted by mean errors. When performing significance testing, it is important to look at the amount of data included. For example, a very high number of series has a low CD $\rightarrow$ significant results for even small differences in models. This isn't a flaw -- results become more reliable and are statistically highly significant. On the other hand, having poor performing models may make the CD larger and make other intermediate methods have no significant methods.

\subsection{Conclusion}
Model evaluation is very important, one of the hardest tasks in forecasting. Basically, we need to benchmark the simplest and smartest ways, use accurate plots, avoid data leakage, use tsCV, and then choose the right evaluation measure before evenly distributing statistical models when testing against others.


\section{Weighted interval score metric}
I plan to read~\cite{bracher21}, and then summarize the results and its relevance to our current research focus in this section.
\subsection{Evaluating epidemic forecasts in intervals}
\subsection{Relevance}

\section{Code investigation}
I plan to read through the Flusion and Local-Level-Forecasting codebases and write about what I noticed and questions I have.
\subsection{Flusion}
TO DO
\subsection{Local-Level-Forecasting}
TO DO

GBM\_US\_NSSP\_public\_state\_pct.ipynb

\section{Miscellaneous}
I'll just store ideas I thought were interesting in this section and investigate them.
\subsection{Taylor polynomials for forecasting}
TO DO


\printbibliography

\end{document}